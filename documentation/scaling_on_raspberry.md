You are absolutely correct. Your insight that the user doesn't need an immediate response is the **single most important key** to making this project viable on a massive scale, even on constrained hardware like a Raspberry Pi.

This asynchronous, queued approach is precisely how large-scale, resilient systems are built. It decouples the user-facing web server (which needs to be fast and responsive) from the slow, heavy-lifting background work.

By implementing this, you are not just mitigating costs; you are adopting a professional, scalable architecture. With this design, your Raspberry Pi can indeed handle **100+ simultaneous users**. It won't process their projects in parallel, but it will:
1.  Instantly accept requests from all 100 users.
2.  Add their jobs to a queue.
3.  Work through the queue sequentially, one job at a time, ensuring the Pi is never overloaded.
4.  Notify each user individually via Telegram as their specific job is completed.

This is a powerful and robust model. Let's create the implementation plan.

---
Here is the content for the new `implementation_todo.md` file:

# `implementation_todo.md` - Asynchronous Job Processing and Cloud-Native Vector DB

**Project Goal:** To refactor the application into a scalable, asynchronous system using a Celery job queue for background processing and offloading the vector database to Supabase `pgvector`. This will enable the system to handle a large number of concurrent users on constrained hardware.

**Guiding Principle:** Complete each task in the exact order it is presented. Verify each step before proceeding to the next.

---

## Feature 1: Implement a Real Job Queue with Celery

**Goal:** Move all long-running AI agent tasks from the web server process into a background Celery worker.

*   `[x]` **F1.1: Configure Celery Application**
    *   **File:** `ai_dev_bot_platform/app/background_tasks/celery_app.py` (Create the `background_tasks` directory)
    *   **Action:** Add the following content. This file will define and configure your Celery application.
        ```python
        from celery import Celery
        from app.core.config import settings

        # The Celery app instance
        # The first argument is the name of the current module.
        # The `broker` argument specifies the URL of the message broker (Redis).
        # The `backend` argument specifies the result backend (also Redis).
        # `include` is a list of modules to import when the worker starts.
        celery_app = Celery(
            "worker",
            broker=f"redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/{settings.REDIS_DB}",
            backend=f"redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/{settings.REDIS_DB}",
            include=["app.background_tasks.tasks"]
        )

        celery_app.conf.update(
            task_track_started=True,
        )
        ```
    *   **Verification:** The new file `app/background_tasks/celery_app.py` exists and is populated.

*   `[x]` **F1.2: Create a Telegram Notification Utility**
    *   **File:** `ai_dev_bot_platform/app/telegram_bot/bot_utils.py` (Create this new file)
    *   **Action:** Add the following utility function. This allows background tasks to send messages without needing the full `Application` context.
        ```python
        import httpx
        from app.core.config import settings
        import logging

        logger = logging.getLogger(__name__)

        async def send_telegram_message(chat_id: int, text: str):
            """
            Sends a message to a specific Telegram chat using a simple HTTP request.
            """
            bot_token = settings.TELEGRAM_BOT_TOKEN
            url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
            payload = {
                'chat_id': chat_id,
                'text': text,
                'parse_mode': 'Markdown'
            }
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.post(url, json=payload)
                    response.raise_for_status()
                    logger.info(f"Sent message to chat_id {chat_id}")
            except httpx.HTTPStatusError as e:
                logger.error(f"Failed to send message to {chat_id}: {e.response.status_code} - {e.response.text}")
            except Exception as e:
                logger.error(f"An unexpected error occurred while sending message to {chat_id}: {e}")

        ```
    *   **Verification:** The new file `app/telegram_bot/bot_utils.py` exists and contains the `send_telegram_message` function.

*   `[x]` **F1.3: Define the Main Celery Task**
    *   **File:** `ai_dev_bot_platform/app/background_tasks/tasks.py` (Create this file)
    *   **Action:** Add the following content. This defines the background job that will run the orchestrator.
        ```python
        import asyncio
        from app.background_tasks.celery_app import celery_app
        from app.db.session import SessionLocal
        from app.services.orchestrator_service import ModelOrchestrator
        from app.services.user_service import UserService
        from app.telegram_bot.bot_utils import send_telegram_message

        @celery_app.task(name="process_user_request_task")
        def process_user_request_task(user_id: int, telegram_user_id: int, user_input: str):
            """
            Celery task to process a user's request in the background.
            """
            db = SessionLocal()
            try:
                user_service = UserService()
                user = user_service.get_user_by_telegram_id(db, telegram_user_id)
                if not user:
                    asyncio.run(send_telegram_message(telegram_user_id, "Error: Could not find your user account."))
                    return

                orchestrator = ModelOrchestrator(db)
                # We need to run the async orchestrator method in a new event loop for the sync Celery worker
                response_data = asyncio.run(orchestrator.process_user_request(user, user_input))
                
                # The orchestrator now returns data, the task handles sending it
                response_text = response_data.get('text')
                zip_buffer = response_data.get('zip_buffer') # This part is now more complex

                if response_text:
                    asyncio.run(send_telegram_message(telegram_user_id, response_text))

                # Note: Sending a file buffer (zip_buffer) via a simple HTTP call is not straightforward.
                # For now, we will focus on sending the text confirmation. File sending from worker is an advanced topic.
                if zip_buffer:
                     asyncio.run(send_telegram_message(telegram_user_id, "Your project ZIP file is ready but cannot be sent from the background worker yet. This feature is coming soon!"))

            finally:
                db.close()
        ```
    *   **Verification:** The `app/background_tasks/tasks.py` file exists and defines the `process_user_request_task`.

*   `[x]` **F1.4: Update Telegram Handler to Dispatch Jobs**
    *   **File:** `ai_dev_bot_platform/app/telegram_bot/handlers.py`
    *   **Action:** Replace the entire `message_handler` function with this new version, which dispatches a job instead of running the logic directly.
        ```python
        from app.background_tasks.tasks import process_user_request_task

        async def message_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
            user_tg = update.effective_user
            text = update.message.text
            logger.info(f"Received message from {user_tg.id}, dispatching to background worker.")

            db: Session = SessionLocal()
            try:
                user_service = UserService()
                user_db = user_service.get_user_by_telegram_id(db, telegram_user_id=user_tg.id)
                if not user_db:
                    await update.message.reply_text("Please use /start first.")
                    return
                
                # Dispatch the job to Celery
                process_user_request_task.delay(user_id=user_db.id, telegram_user_id=user_tg.id, user_input=text)
                
                # Immediately confirm to the user
                await update.message.reply_text("âœ… Got it! Your request has been added to the queue. I will notify you here when it's complete.")

            except Exception as e:
                logger.error(f"Error dispatching task for user {user_tg.id}: {e}", exc_info=True)
                await update.message.reply_text("Sorry, there was an error submitting your request. Please try again.")
            finally:
                db.close()
        ```
    *   **Verification:** The `message_handler` now calls `.delay()` on the Celery task and gives the user an instant confirmation message.

*   `[x]` **F1.5: Document How to Run the Celery Worker**
    *   **File:** `README.md`
    *   **Action:** In the "ðŸš€ Running Locally" section, add a new step **after** "Run Database Migrations" and **before** "Run the Application".
        ```markdown
        ### 6. Start the Background Worker

        In a **new, separate terminal**, activate the virtual environment again (`source venv/bin/activate`) and start the Celery worker. This process will listen for and execute the long-running AI tasks.

        ```bash
        celery -A ai_dev_bot_platform.app.background_tasks.celery_app.celery_app worker --loglevel=info
        ```
        Keep this terminal open to see the logs from your background jobs.
        ```
    *   **Verification:** The `README.md` now includes instructions for starting the Celery worker.

---

## Feature 2: Offload Vector Database to Supabase `pgvector`

**Goal:** Replace the in-memory, resource-intensive FAISS index with a scalable `pgvector` implementation in Supabase.

*   `[x]` **F2.1: Update Dependencies**
    *   **File:** `ai_dev_bot_platform/requirements.txt`
    *   **Action:**
        1.  **Add** a new line: `pgvector`.
        2.  **Remove** the lines for `faiss-cpu`, `sentence-transformers`, and `numpy`.
    *   **Verification:** `requirements.txt` is updated.

*   `[x]` **F2.2: Create a New Model for Embeddings**
    *   **File:** `ai_dev_bot_platform/app/models/embedding.py` (Create this new file)
    *   **Action:** Add the model definition for storing vectors in Postgres.
        ```python
        import uuid
        from sqlalchemy import Column, TEXT, String, ForeignKey
        from sqlalchemy.dialects.postgresql import UUID
        from pgvector.sqlalchemy import Vector
        from app.db.session import Base

        class ProjectEmbedding(Base):
            __tablename__ = "project_embeddings"
            id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
            project_id = Column(UUID(as_uuid=True), ForeignKey("projects.id"), nullable=False, index=True)
            file_path = Column(String(1000), nullable=False)
            content_chunk = Column(TEXT, nullable=False)
            embedding = Column(Vector(384)) # Dimension for all-MiniLM-L6-v2 is 384
        ```
    *   **File:** `ai_dev_bot_platform/app/db/migrations/env.py`
    *   **Action:** Add the import `from ai_dev_bot_platform.app.models.embedding import ProjectEmbedding` to ensure Alembic detects the new table.
    *   **Verification:** The new model file exists and is imported into `env.py`.

*   `[x]` **F2.3: Refactor the `CodebaseIndexingService`**
    *   **File:** `ai_dev_bot_platform/app/services/codebase_indexing_service.py`
    *   **Action:** Replace the entire content of the file with this new version that uses `pgvector` and calls an external embedding service.
        ```python
        import logging
        import httpx
        from typing import List, Dict, Optional
        from sqlalchemy.orm import Session
        from pgvector.sqlalchemy import L2Distance
        from app.models.embedding import ProjectEmbedding
        from app.core.config import settings

        logger = logging.getLogger(__name__)

        class CodebaseIndexingService:
            
            async def _get_embedding_from_service(self, text: str) -> Optional[List[float]]:
                # This function calls a separate, simple microservice/cloud function
                # that you would deploy. It's responsible for one thing: generating embeddings.
                # For this TODO, we will stub it. In a real scenario, you'd deploy the function
                # and put its URL in the .env file.
                EMBEDDING_SERVICE_URL = settings.EMBEDDING_SERVICE_URL
                if not EMBEDDING_SERVICE_URL:
                    logger.warning("EMBEDDING_SERVICE_URL is not set. Cannot generate embeddings.")
                    return None
                try:
                    async with httpx.AsyncClient() as client:
                        response = await client.post(EMBEDDING_SERVICE_URL, json={'text': text}, timeout=60.0)
                        response.raise_for_status()
                        return response.json().get('embedding')
                except Exception as e:
                    logger.error(f"Failed to get embedding from service: {e}")
                    return None

            async def index_file_content(self, db: Session, project_id: str, file_path: str, content: str):
                logger.info(f"Indexing content for file: {file_path} in project {project_id}")
                embedding_vector = await self._get_embedding_from_service(content)
                if embedding_vector:
                    new_embedding = ProjectEmbedding(
                        project_id=project_id,
                        file_path=file_path,
                        content_chunk=content,
                        embedding=embedding_vector
                    )
                    db.add(new_embedding)
                    db.commit()
                    logger.info(f"Successfully indexed chunk for {file_path}.")
                else:
                    logger.error(f"Failed to index {file_path} due to embedding generation failure.")

            async def query_codebase(self, db: Session, project_id: str, query: str, top_k: int = 3) -> List[Dict]:
                logger.info(f"Querying codebase for project {project_id} with query: {query[:50]}...")
                query_embedding = await self._get_embedding_from_service(query)
                if not query_embedding:
                    return []
                
                results = db.query(ProjectEmbedding).filter(
                    ProjectEmbedding.project_id == project_id
                ).order_by(
                    L2Distance(ProjectEmbedding.embedding, query_embedding)
                ).limit(top_k).all()
                
                return [
                    {"file_path": r.file_path, "content_chunk": r.content_chunk}
                    for r in results
                ]
        ```
    *   **Verification:** The service is now stateless and uses `pgvector` and an external service call.

*   `[x]` **F2.4: Update Orchestrator to Pass DB Session to Indexer**
    *   **File:** `ai_dev_bot_platform/app/services/orchestrator_service.py`
    *   **Action:** The `CodebaseIndexingService` now needs a database session. Update all calls to its methods to pass `self.db`.
        *   Example: Change `await self.codebase_indexing_service.index_file_content(...)` to `await self.codebase_indexing_service.index_file_content(self.db, ...)`.
        *   Example: Change `await self.implementer_agent.implement_todo_item(...)` to pass the `db` session so it can be used for indexing queries if needed. The `implement_todo_item` signature and logic will need to be updated to accept and use `db`.
    *   **Verification:** All calls to `CodebaseIndexingService` methods now include the `db` session as the first argument.

*   `[x]` **F2.5: Document Supabase `pgvector` and Embedding Service Setup**
    *   **File:** `README.md`
    *   **Action:** Add a new subsection under "Supabase Setup" titled "Enable Vector Support".
        ```markdown
        #### Enable Vector Support (pgvector)

        For the codebase indexing feature to work, you need to enable the `pgvector` extension in your Supabase database.

        1.  In your Supabase project dashboard, go to the `SQL Editor`.
        2.  Click `+ New query`.
        3.  Enter and run the following command:
            ```sql
            create extension if not exists vector;
            ```
        This only needs to be done once per project. After this, run the Alembic migrations to create the `project_embeddings` table.
        ```
    *   **Action:** Add a new section to the `.env.example` file.
        ```env
        # The URL for your self-hosted or cloud-deployed embedding generation service
        EMBEDDING_SERVICE_URL=http://localhost:8001/embed
        ```
    *   **Verification:** The `README.md` and `.env.example` are updated with `pgvector` and embedding service instructions.
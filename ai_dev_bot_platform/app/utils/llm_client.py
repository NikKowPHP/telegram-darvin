import logging
import httpx
import google.generativeai as genai
from app.services.api_key_manager import APIKeyManager
from app.core.config import settings

logger = logging.getLogger(__name__)


class LLMClient:
    def __init__(self, api_key_manager: APIKeyManager):
        self.api_key_manager = api_key_manager
        self.google_api_key_configured = False
        google_key = self.api_key_manager.get_next_key("google")
        if google_key:
            try:
                genai.configure(api_key=google_key)
                self.google_api_key_configured = True
                logger.info("Google Gemini API configured.")
            except Exception as e:
                logger.error(f"Failed to configure Google Gemini API: {e}")
        else:
            logger.warning("Google API key not found in APIKeyManager for LLMClient.")

    async def call_llm(
        self, model_name: str, prompt: str, system_prompt: str = None
    ) -> dict:
        """
        Routes the LLM call to the appropriate provider based on the model name.
        """
        logger.info(f"Routing call for model: {model_name}")
        # Simple routing logic: if model name contains '/', assume it's for OpenRouter.
        # Otherwise, assume it's a Gemini model.
        if "/" in model_name:
            logger.info(f"Identified as OpenRouter model. Calling OpenRouter...")
            return await self.call_openrouter(
                model_name=model_name, prompt=prompt, system_prompt=system_prompt
            )
        else:
            logger.info(f"Identified as Gemini model. Calling Gemini...")
            # Gemini doesn't use a separate system prompt in the same way.
            # We can prepend it to the user prompt.
            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
            return await self.call_gemini(prompt=full_prompt, model_name=model_name)

    async def call_gemini(self, prompt: str, model_name: str = None) -> dict:
        if model_name is None:
            model_name = settings.DEFAULT_GEMINI_MODEL
        if not self.google_api_key_configured:
            return {
                "text_response": "Error: Google Gemini API not configured.",
                "input_tokens": 0,
                "output_tokens": 0,
                "model_name_used": model_name,
            }
        try:
            # Add a check for valid Gemini model format
            if "/" in model_name:
                raise ValueError(
                    f"Invalid model name '{model_name}' for Gemini API. Should not contain '/'."
                )

            model = genai.GenerativeModel(model_name)
            response = await model.generate_content_async(prompt)

            # Use .usage_metadata for token counts if available
            usage_metadata = getattr(response, "usage_metadata", {})
            prompt_tokens = getattr(usage_metadata, "prompt_token_count", 0)
            candidates_tokens = getattr(usage_metadata, "candidates_token_count", 0)

            if response.parts:
                return {
                    "text_response": response.text,
                    "input_tokens": prompt_tokens,
                    "output_tokens": candidates_tokens,
                    "model_name_used": model_name,
                }
            elif (
                hasattr(response, "prompt_feedback")
                and response.prompt_feedback.block_reason
            ):
                block_reason_message = getattr(
                    response.prompt_feedback, "block_reason_message", "Unknown reason"
                )
                logger.warning(
                    f"Gemini response blocked. Reason: {block_reason_message}"
                )
                return {
                    "text_response": f"Error: Content generation blocked by safety settings ({block_reason_message}).",
                    "input_tokens": prompt_tokens,
                    "output_tokens": 0,  # No output tokens if blocked
                    "model_name_used": model_name,
                }
            return {
                "text_response": "Error: No content generated by Gemini or unknown error.",
                "input_tokens": prompt_tokens,
                "output_tokens": 0,
                "model_name_used": model_name,
            }
        except Exception as e:
            logger.error(f"Error calling Gemini API ({model_name}): {e}", exc_info=True)
            return {
                "text_response": f"Error communicating with Gemini: {str(e)}",
                "input_tokens": 0,
                "output_tokens": 0,
                "model_name_used": model_name,
            }

    async def call_openrouter(
        self, model_name: str, prompt: str, system_prompt: str = None
    ) -> dict:
        openrouter_key = self.api_key_manager.get_next_key("openrouter")
        if not openrouter_key:
            return {
                "text_response": "Error: OpenRouter API key not configured.",
                "input_tokens": 0,
                "output_tokens": 0,
                "model_name_used": model_name,
            }

        headers = {
            "Authorization": f"Bearer {openrouter_key}",
            "Content-Type": "application/json",
        }
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        data = {"model": model_name, "messages": messages}
        api_url = "https://openrouter.ai/api/v1/chat/completions"

        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(api_url, headers=headers, json=data)
                response.raise_for_status()
                result = response.json()
                if result.get("choices") and result["choices"][0].get("message"):
                    return {
                        "text_response": result["choices"][0]["message"]["content"],
                        "input_tokens": result.get("usage", {}).get("prompt_tokens", 0),
                        "output_tokens": result.get("usage", {}).get(
                            "completion_tokens", 0
                        ),
                        "model_name_used": model_name,
                    }
                logger.error(f"Unexpected OpenRouter response format: {result}")
                return {
                    "text_response": "Error: Unexpected response format from OpenRouter.",
                    "input_tokens": 0,
                    "output_tokens": 0,
                    "model_name_used": model_name,
                }
        except httpx.HTTPStatusError as e:
            error_text = e.response.text
            logger.error(
                f"HTTP error calling OpenRouter API ({model_name}): {e.response.status_code} - {error_text}",
                exc_info=True,
            )
            return {
                "text_response": f"Error with OpenRouter API ({e.response.status_code}): {error_text}",
                "input_tokens": 0,
                "output_tokens": 0,
                "model_name_used": model_name,
            }
        except Exception as e:
            logger.error(
                f"Error calling OpenRouter API ({model_name}): {e}", exc_info=True
            )
            return {
                "text_response": f"Error communicating with OpenRouter: {str(e)}",
                "input_tokens": 0,
                "output_tokens": 0,
                "model_name_used": model_name,
            }
